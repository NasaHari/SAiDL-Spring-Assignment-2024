{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport os\nimport enum\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom networkx.readwrite import json_graph\n\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport zipfile\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-03-23T13:26:52.358675Z","iopub.execute_input":"2024-03-23T13:26:52.359081Z","iopub.status.idle":"2024-03-23T13:26:57.969877Z","shell.execute_reply.started":"2024-03-23T13:26:52.359055Z","shell.execute_reply":"2024-03-23T13:26:57.969060Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"DATA_DIR_PATH = os.path.join(os.getcwd(), 'data')\nPPI_PATH = os.path.join(DATA_DIR_PATH, 'ppi')\nPPI_URL = 'https://data.dgl.ai/dataset/ppi.zip'  \n\n\n# \\\\ constants\n\n\nPPI_NUM_INPUT_FEATURES = 50\nPPI_NUM_CLASSES = 121\nPPI_PATH","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:26:57.971046Z","iopub.execute_input":"2024-03-23T13:26:57.971519Z","iopub.status.idle":"2024-03-23T13:26:57.980295Z","shell.execute_reply.started":"2024-03-23T13:26:57.971493Z","shell.execute_reply":"2024-03-23T13:26:57.979495Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/data/ppi'"},"metadata":{}}]},{"cell_type":"code","source":"def json_read(path):\n    with open(path, 'r') as file:\n        data = json.load(file)\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:26:57.982798Z","iopub.execute_input":"2024-03-23T13:26:57.983098Z","iopub.status.idle":"2024-03-23T13:26:57.996121Z","shell.execute_reply.started":"2024-03-23T13:26:57.983074Z","shell.execute_reply":"2024-03-23T13:26:57.995261Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def load_graph_data(training_config, device):\n    \n    dataset_name = training_config['dataset_name'].lower()\n\n    if not os.path.exists(PPI_PATH):  # download the first time this is ran\n        os.makedirs(PPI_PATH)\n\n        zip_tmp_path = os.path.join(PPI_PATH, 'ppi.zip')\n        torch.hub.download_url_to_file(PPI_URL, zip_tmp_path)\n\n        with zipfile.ZipFile(zip_tmp_path) as zf:\n            zf.extractall(path=PPI_PATH)\n        print(f'Unzipping to: {PPI_PATH} finished.')\n\n        os.remove(zip_tmp_path)\n        print(f'Removing tmp file {zip_tmp_path}.')\n\n    edge_index_list = []\n    node_features_list = []\n    node_labels_list = []\n    num_graphs_per_split_cumulative = [0]\n\n    splits = ['test'] if training_config['ppi_load_test_only'] else ['train', 'valid', 'test']\n\n    for split in splits:\n        node_features = np.load(os.path.join(PPI_PATH, f'{split}_feats.npy'))\n        node_labels = np.load(os.path.join(PPI_PATH, f'{split}_labels.npy'))\n        nodes_links_dict = json_read(os.path.join(PPI_PATH, f'{split}_graph.json'))\n        \n        # Convert undirected graph into directed\n        collection_of_graphs = nx.DiGraph(json_graph.node_link_graph(nodes_links_dict))\n        \n        # For each node in the above collection, ids specify to which graph the node belongs to\n        graph_ids = np.load(os.path.join(PPI_PATH, F'{split}_graph_id.npy'))\n        num_graphs_per_split_cumulative.append(num_graphs_per_split_cumulative[-1] + len(np.unique(graph_ids)))\n\n        for graph_id in range(np.min(graph_ids), np.max(graph_ids) + 1):\n            mask = graph_ids == graph_id  \n            graph_node_ids = np.asarray(mask).nonzero()[0]\n            graph = collection_of_graphs.subgraph(graph_node_ids)  \n            print(f'Loading {split} graph {graph_id} to CPU. '\n                  f'It has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.')\n\n            edge_index = torch.tensor(list(graph.edges), dtype=torch.long).transpose(0, 1).contiguous()\n            edge_index = edge_index - edge_index.min()  \n            edge_index_list.append(edge_index)\n\n            node_features_list.append(torch.tensor(node_features[mask], dtype=torch.float))\n            node_labels_list.append(torch.tensor(node_labels[mask], dtype=torch.float))\n\n    if training_config['ppi_load_test_only']:\n        data_loader_test = GraphDataLoader(\n            node_features_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n            node_labels_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n            edge_index_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n            batch_size=training_config['batch_size'],\n            shuffle=False\n        )\n        return data_loader_test\n    else:\n        data_loader_train = GraphDataLoader(\n            node_features_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n            node_labels_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n            edge_index_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n            batch_size=training_config['batch_size'],\n            shuffle=True\n        )\n\n        data_loader_val = GraphDataLoader(\n            node_features_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],\n            node_labels_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],\n            edge_index_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],\n            batch_size=training_config['batch_size'],\n            shuffle=False  \n        )\n\n        data_loader_test = GraphDataLoader(\n            node_features_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],\n            node_labels_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],\n            edge_index_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],\n            batch_size=training_config['batch_size'],\n            shuffle=False\n        )\n\n        return data_loader_train, data_loader_val, data_loader_test\n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:26:57.997366Z","iopub.execute_input":"2024-03-23T13:26:57.997638Z","iopub.status.idle":"2024-03-23T13:26:58.017784Z","shell.execute_reply.started":"2024-03-23T13:26:57.997616Z","shell.execute_reply":"2024-03-23T13:26:58.016961Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class GraphDataLoader(DataLoader):\n   \n    def __init__(self, node_features_list, node_labels_list, edge_index_list, batch_size=1, shuffle=False):\n        graph_dataset = GraphDataset(node_features_list, node_labels_list, edge_index_list)\n        super().__init__(graph_dataset, batch_size, shuffle, collate_fn=graph_collate_fn)\n\n\nclass GraphDataset(Dataset):\n  \n    def __init__(self, node_features_list, node_labels_list, edge_index_list):\n        self.node_features_list = node_features_list\n        self.node_labels_list = node_labels_list\n        self.edge_index_list = edge_index_list\n\n    def __len__(self):\n        return len(self.edge_index_list)\n\n    def __getitem__(self, idx):  # we just fetch a single graph\n        return self.node_features_list[idx], self.node_labels_list[idx], self.edge_index_list[idx]\n\n\ndef graph_collate_fn(batch):\n\n\n    edge_index_list = []\n    node_features_list = []\n    node_labels_list = []\n    num_nodes_seen = 0\n\n    for features_labels_edge_index in batch:\n        # Just collect these into separate lists\n        node_features_list.append(features_labels_edge_index[0])\n        node_labels_list.append(features_labels_edge_index[1])\n\n        edge_index = features_labels_edge_index[2]  \n        edge_index_list.append(edge_index + num_nodes_seen) #So if we've processed 10 nodes from previous graphs, an edge index of (1, 3) from a new graph becomes (11, 13) in the batch-wide representation.\n        num_nodes_seen += len(features_labels_edge_index[1])  \n\n    node_features = torch.cat(node_features_list, 0)\n    node_labels = torch.cat(node_labels_list, 0)\n    edge_index = torch.cat(edge_index_list, 1)\n\n    return node_features, node_labels, edge_index","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:26:58.019495Z","iopub.execute_input":"2024-03-23T13:26:58.019748Z","iopub.status.idle":"2024-03-23T13:26:58.032129Z","shell.execute_reply.started":"2024-03-23T13:26:58.019726Z","shell.execute_reply":"2024-03-23T13:26:58.031289Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU\nconfig = {\n    'dataset_name': \"PPI\",\n    'batch_size': 1,\n    'ppi_load_test_only': False  # small optimization for loading test graphs only, we won't use it here\n}","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:30:47.965165Z","iopub.execute_input":"2024-03-23T13:30:47.966133Z","iopub.status.idle":"2024-03-23T13:30:48.027458Z","shell.execute_reply.started":"2024-03-23T13:30:47.966097Z","shell.execute_reply":"2024-03-23T13:30:48.026288Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data_loader_train, data_loader_val, data_loader_test = load_graph_data(config, device)\n# Let's fetch a single batch from the train graph data loader\nnode_features, node_labels, edge_index = next(iter(data_loader_train))\n\nprint('*' * 20)\nprint(node_features.shape, node_features.dtype)\nprint(node_labels.shape, node_labels.dtype)\nprint(edge_index.shape, edge_index.dtype)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:30:48.165443Z","iopub.execute_input":"2024-03-23T13:30:48.166175Z","iopub.status.idle":"2024-03-23T13:31:12.091347Z","shell.execute_reply.started":"2024-03-23T13:30:48.166141Z","shell.execute_reply":"2024-03-23T13:31:12.090274Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 6.76M/6.76M [00:00<00:00, 78.4MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Unzipping to: /kaggle/working/data/ppi finished.\nRemoving tmp file /kaggle/working/data/ppi/ppi.zip.\nLoading train graph 1 to CPU. It has 1767 nodes and 34085 edges.\nLoading train graph 2 to CPU. It has 1377 nodes and 31081 edges.\nLoading train graph 3 to CPU. It has 2263 nodes and 61907 edges.\nLoading train graph 4 to CPU. It has 2339 nodes and 67769 edges.\nLoading train graph 5 to CPU. It has 1578 nodes and 37740 edges.\nLoading train graph 6 to CPU. It has 1021 nodes and 19237 edges.\nLoading train graph 7 to CPU. It has 1823 nodes and 46153 edges.\nLoading train graph 8 to CPU. It has 2488 nodes and 72878 edges.\nLoading train graph 9 to CPU. It has 591 nodes and 8299 edges.\nLoading train graph 10 to CPU. It has 3312 nodes and 109510 edges.\nLoading train graph 11 to CPU. It has 2401 nodes and 66619 edges.\nLoading train graph 12 to CPU. It has 1878 nodes and 48146 edges.\nLoading train graph 13 to CPU. It has 1819 nodes and 47587 edges.\nLoading train graph 14 to CPU. It has 3480 nodes and 110234 edges.\nLoading train graph 15 to CPU. It has 2794 nodes and 88112 edges.\nLoading train graph 16 to CPU. It has 2326 nodes and 62188 edges.\nLoading train graph 17 to CPU. It has 2650 nodes and 79714 edges.\nLoading train graph 18 to CPU. It has 2815 nodes and 88335 edges.\nLoading train graph 19 to CPU. It has 3163 nodes and 97321 edges.\nLoading train graph 20 to CPU. It has 3021 nodes and 94359 edges.\nLoading valid graph 21 to CPU. It has 3230 nodes and 100676 edges.\nLoading valid graph 22 to CPU. It has 3284 nodes and 104758 edges.\nLoading test graph 23 to CPU. It has 3224 nodes and 103872 edges.\nLoading test graph 24 to CPU. It has 2300 nodes and 63628 edges.\n********************\ntorch.Size([2794, 50]) torch.float32\ntorch.Size([2794, 121]) torch.float32\ntorch.Size([2, 88112]) torch.int64\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.optim import Adam\n\n\nclass GAT(torch.nn.Module):\n   \n\n    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection=True, bias=True,\n                 dropout=0.6, log_attention_weights=False):\n        super().__init__()\n        assert num_of_layers == len(num_heads_per_layer) == len(num_features_per_layer) - 1, f'Enter valid  params.'\n\n        num_heads_per_layer = [1] + num_heads_per_layer  #first layer of the GAT model, the input data typically does not have any hierarchical representations or higher-level features that require multiple attention heads to capture.\n        gat_layers = []  \n        for i in range(num_of_layers):\n            layer = GATLayer(\n                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  \n                num_out_features=num_features_per_layer[i+1],\n                num_of_heads=num_heads_per_layer[i+1],\n                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat\n                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores\n                dropout_prob=dropout,\n                add_skip_connection=add_skip_connection,\n                bias=bias,\n                log_attention_weights=log_attention_weights\n            )\n            gat_layers.append(layer)\n\n        self.gat_net = nn.Sequential(\n            *gat_layers,\n        )\n\n    # data is just a (in_nodes_features, edge_index) tuple, I had to do it like this because of the nn.Sequential:\n    # https://discuss.pytorch.org/t/forward-takes-2-positional-arguments-but-3-were-given-for-nn-sqeuential-with-linear-layers/65698\n    def forward(self, data):\n        return self.gat_net(data)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:32:01.303420Z","iopub.execute_input":"2024-03-23T13:32:01.303795Z","iopub.status.idle":"2024-03-23T13:32:01.313851Z","shell.execute_reply.started":"2024-03-23T13:32:01.303765Z","shell.execute_reply":"2024-03-23T13:32:01.312658Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class GATLayer(torch.nn.Module):\n    \n\n    src_nodes_dim = 0  # position of source nodes in edge index\n    trg_nodes_dim = 1  # position of target nodes in edge index\n\n   \n    nodes_dim = 0     \n    head_dim = 1       \n\n    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n\n        super().__init__()\n\n        self.num_of_heads = num_of_heads\n        self.num_out_features = num_out_features\n        self.concat = concat  # whether we should concatenate or average the attention heads\n        self.add_skip_connection = add_skip_connection\n\n        \n        \n        \n\n        #  num_of_heads independent W matrices\n        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n\n        # In the GAT (Graph Attention Network) model, attention scores are computed between pairs of nodes in the graph to determine the importance of each neighbor node to a target node. These attention scores are crucial for aggregating information from neighboring nodes during message passing.\n        #\n        # In the GAT paper, attention scores are calculated using an additive attention mechanism. Specifically, for each attention head, the attention score between a target node (node i) and a source node (node j) is computed as follows:\n        #\n        #   score_ij = LeakyReLU( (W * hi) ⋅ (atgt)^T + (W * hj) ⋅ (asrc)^T )\n        #\n        # Here:\n        #   - W represents the learnable parameters of the linear projection, transforming the input node features hi and hj into the desired output dimension num_out_features.\n        #   - atgt and asrc are learnable parameters specific to the target and source nodes, respectively. These parameters are represented by self.scoring_fun_target and self.scoring_fun_source.\n        #   - LeakyReLU is the activation function used in the scoring function.\n        #\n        # The self.scoring_fun_target parameter is defined as a learnable tensor of shape (1, num_of_heads, num_out_features). This tensor holds the parameters atgt used in the scoring function for targeting nodes. Each attention head has its own set of parameters, allowing the model to learn different attention patterns across heads.\n        #\n        # By parameterizing the scoring function in this way, the GAT model can learn to assign different importance scores to neighbors of each target node, enabling effective information aggregation in graph-structured data. The parameters in self.scoring_fun_target are updated during training via backpropagation, allowing the model to adaptively learn the attention mechanism that best suits the task at hand.\n\n        self.scoring_fun_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n        self.scoring_fun_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n\n        if bias and concat:\n            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\n        elif bias and not concat:\n            self.bias = nn.Parameter(torch.Tensor(num_out_features))\n        else:\n            self.register_parameter('bias', None)\n\n        if add_skip_connection:#skip connections in the GAT model are like shortcuts that allow the network to directly use the original input features along with the processed features from the current layer. This helps the model retain valuable information from the input, making it easier to learn and preventing the loss of important information during training.\n            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n        else:\n            self.register_parameter('skip_proj', None)\n\n        self.leakyReLU = nn.LeakyReLU(0.2) \n        self.activation = activation\n    \n        self.dropout = nn.Dropout(p=dropout_prob)\n\n        self.log_attention_weights = log_attention_weights  \n        self.attention_weights =None \n\n        self.init_params()\n\n    def forward(self, data):\n       \n\n        in_nodes_features, edge_index = data  # unpack data\n        ''' in_nodes_features: This tensor represents the input node\n        features. It has a shape of (N, F), where N is the number of \n        nodes and F is the number of features per node. Each row \n        corresponds to the feature vector of a single node in the graph.\n        edge_index: This tensor represents the edge index of the \n        graph. It has a shape of (2, E), where E is the number of\n        edges in the graph. Each column corresponds to an edge in \n        the graph, where the first row contains the indices of the\n        source nodes and the second row contains the indices of the\n        target nodes for each edge.'''\n        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\n\n        # shape = (N, F_input) \n        # \n        in_nodes_features = self.dropout(in_nodes_features)\n\n        # shape = (N, F_input) * (F_input, NoHeads*F_output) -> (N, NoHeads, F_output) \n        # We project the input node features into Number of Heads independent output features (one for each attention head)\n        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n\n        nodes_features_proj = self.dropout(nodes_features_proj)  \n\n        \n        #  Edge attention calculation\n        \n\n        # Apply the scoring function (* represents element-wise (Hadamard) product)\n        # shape = (N, NoHeads, F_output)[nodes_features_proj] * [scoring_function](1, NNoHeadsH, F_output) -> (N, NoHeads, 1) -> (N, NoHeads) becaus\n        scores_source = (nodes_features_proj * self.scoring_fun_source).sum(dim=-1)\n        scores_target = (nodes_features_proj * self.scoring_fun_target).sum(dim=-1)\n\n        \n        # not all the possible combinations of scores  just prepare scores that will actually be used and those are defined\n        # by the edge index.\n        # scores shape = (E, NoHeads), nodes_features_proj_lifted shape = (E, NoHeads, F_output), E - number of edges in the graph\n        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\n\n        # shape = (E, NoHeads , 1)\n        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n        attentions_per_edge = self.dropout(attentions_per_edge)\n\n        #\n        # Step 3: Neighborhood aggregation\n        #\n\n        # Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul\n        # shape = (E, NoHeads , F_output) * (E, NoHeads , 1) -> (E, NoHeads , F_output), 1 gets broadcast into F_output\n        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n\n        # shape = (N, NoHeads , F_output)\n        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n\n\n        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n        return (out_nodes_features, edge_index)\n\n    #\n    # \n    #\n\n    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):\n       \n        '''ensures that all exponentials are computed with non-positive inputs, which results in values between 0 and 1 after exponentiation.'''\n        scores_per_edge = scores_per_edge - scores_per_edge.max()\n        exp_scores_per_edge = scores_per_edge.exp()  # softmax\n\n        # . shape = (E, NoHeads)\n        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)\n\n        \n        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)\n\n        # shape = (E, NoHeads) -> (E, NoHeads, 1) \n        '''eg Original attention scores:\ntensor([[0.8000, 0.2000],\n        [0.6000, 0.4000],\n        [0.3000, 0.7000]])\nReshaped attention scores:\ntensor([[[0.8000],\n         [0.2000]],\n\n        [[0.6000],\n         [0.4000]],\n\n        [[0.3000],\n         [0.7000]]])'''\n        return attentions_per_edge.unsqueeze(-1)\n\n    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):\n        # The shape must be the same as in exp_scores_per_edge (required by scatter_add_) i.e. from E -> (E, NoHeads )\n        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)\n\n        # shape = (N, NoHeads ), where N is the number of nodes and NoHeads the number of attention heads\n        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible\n        size[self.nodes_dim] = num_of_nodes\n        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\n\n        # position i will contain a sum of exp scores of all the nodes that point to the node i (as dictated by the\n        # target index)\n        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\n\n        # Expand again so that we can use it as a softmax denominator. e.g. node i's sum will be copied to\n        # all the locations where the source nodes pointed to i (as dictated by the target index)\n        # shape = (N, NoHeads ) -> (E, NoHeads )\n        return neighborhood_sums.index_select(self.nodes_dim, trg_index)\n\n    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\n        size = list(nodes_features_proj_lifted_weighted.shape)  # convert to list otherwise assignment is not possible\n        size[self.nodes_dim] = num_of_nodes  # shape = (N, NoHeads , F_output)\n        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)\n\n        # shape = (E) -> (E, NoHeads , F_output)\n        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted)\n        # shape = (E, NoHeads , F_output) -> (N, NoHeads , F_output)\n        '''For each pair of indices (i, j) in trg_index_broadcasted, we take the corresponding values from\nnodes_features_proj_lifted_weighted and add them to out_nodes_features at the specified positions (i, j).\nThis addition is done in such a way that if multiple values are added to the same position (i, j), they are accumulated (added together).'''\n        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\n\n        return out_nodes_features\n\n    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n        '''for  the first edge (0, 1),  extracting \n        scores_source[0], scores_target[1], and nodes_features_matrix_proj[0].\n        '''\n        \n        src_nodes_index = edge_index[self.src_nodes_dim]\n        trg_nodes_index = edge_index[self.trg_nodes_dim]\n\n        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n        nodes_features_matrix_projection_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n\n        return scores_source, scores_target, nodes_features_matrix_projection_lifted\n\n    def explicit_broadcast(self, this, other):\n        for _ in range(this.dim(), other.dim()):\n            this = this.unsqueeze(-1)\n\n        # Explicitly expand so that shapes are the same\n        return this.expand_as(other)\n\n    def init_params(self):\n        \"\"\"\n        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n\n        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n        Feel free to experiment - there may be better initializations depending on your problem.\n\n        \"\"\"\n        nn.init.xavier_uniform_(self.linear_proj.weight)\n        nn.init.xavier_uniform_(self.scoring_fun_target)\n        nn.init.xavier_uniform_(self.scoring_fun_source)\n\n        if self.bias is not None:\n            torch.nn.init.zeros_(self.bias)\n\n    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):\n       \n\n        if self.add_skip_connection:  # add skip or residual connection\n            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:  # if F_input  == F_output\n                # unsqueeze does this: (N, F_input ) -> (N, 1, F_input ), out features are (N, NoHeads , F_output) so 1 gets broadcast to NoHeads \n                # thus we're basically copying input vectors NoHeads times and adding to processed vectors\n                out_nodes_features += in_nodes_features.unsqueeze(1)\n            else:\n                # F_input  != F_output so we need to project input feature vectors into dimension that can be added to output\n                # feature vectors. skip_proj adds lots of additional capacity which may cause overfitting.\n                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n\n        if self.concat:\n            # shape = (N, NoHeads , F_output) -> (N, NoHeads *F_output)\n            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)\n        else:\n            # shape = (N, NoHeads , F_output) -> (N, F_output)\n            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\n\n        if self.bias is not None:\n            out_nodes_features += self.bias\n\n        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:32:01.679384Z","iopub.execute_input":"2024-03-23T13:32:01.679758Z","iopub.status.idle":"2024-03-23T13:32:01.714633Z","shell.execute_reply.started":"2024-03-23T13:32:01.679729Z","shell.execute_reply":"2024-03-23T13:32:01.713639Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\n\n\nclass LoopPhase(enum.Enum):\n    TRAIN = 0,\n    VAL = 1,\n    TEST = 2\n\n\nwriter = SummaryWriter()  # (tensorboard) writer will output to ./runs/ directory by default\n\n\n\nBEST_VAL_MICRO_F1 = 0\nBEST_VAL_LOSS = 0\nPATIENCE_CNT = 0\n\nCHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints')\n\n# Make sure these exist as the rest of the code assumes it\nos.makedirs(BINARIES_PATH, exist_ok=True)\nos.makedirs(CHECKPOINTS_PATH, exist_ok=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:33:28.368571Z","iopub.execute_input":"2024-03-23T13:33:28.369592Z","iopub.status.idle":"2024-03-23T13:33:28.377240Z","shell.execute_reply.started":"2024-03-23T13:33:28.369554Z","shell.execute_reply":"2024-03-23T13:33:28.376371Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Testing  the GAT model\nnum_of_layers = 2  # Example: Number of GAT layers\nnum_heads_per_layer = [4, 2]  # Example: Number of attention heads per layer\nnum_features_per_layer = [node_features.shape[1], 32, 16]  # Example: Number of features per layer\nadd_skip_connection = True\nbias = True\ndropout = 0.6\nlog_attention_weights = False\n\ngat_model = GAT(num_of_layers=num_of_layers,\n                num_heads_per_layer=num_heads_per_layer,\n                num_features_per_layer=num_features_per_layer,\n                add_skip_connection=add_skip_connection,\n                bias=bias,\n                dropout=dropout,\n                log_attention_weights=log_attention_weights)\n\n# Pass a batch of graph data through the model\noutput = gat_model((node_features, edge_index))\n\n# Analyze the output\nout_nodes_features, edge_index = output\nprint(\"Output node features shape:\", out_nodes_features.shape)\nprint(\"Edge index shape:\", edge_index.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:32:16.280093Z","iopub.execute_input":"2024-03-23T13:32:16.280800Z","iopub.status.idle":"2024-03-23T13:32:16.525065Z","shell.execute_reply.started":"2024-03-23T13:32:16.280769Z","shell.execute_reply":"2024-03-23T13:32:16.523979Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Output node features shape: torch.Size([2794, 16])\nEdge index shape: torch.Size([2, 88112])\n","output_type":"stream"}]},{"cell_type":"code","source":"import re  # regex\n\n\ndef get_training_state(training_config, model):\n    training_state = {\n       \n        \"num_of_epochs\": training_config['num_of_epochs'],\n        \"test_perf\": training_config['test_perf'],\n\n        \"num_of_layers\": training_config['num_of_layers'],\n        \"num_heads_per_layer\": training_config['num_heads_per_layer'],\n        \"num_features_per_layer\": training_config['num_features_per_layer'],\n        \"add_skip_connection\": training_config['add_skip_connection'],\n        \"bias\": training_config['bias'],\n        \"dropout\": training_config['dropout'],\n\n        #\n        \"state_dict\": model.state_dict()\n    }\n\n    return training_state\n\n\ndef print_model_metadata(training_state):\n    header = f'\\n{\"*\"*5} Model training metadata: {\"*\"*5}'\n    print(header)\n\n    for key, value in training_state.items():\n        if key != 'state_dict':  # don't print state_dict it's a bunch of numbers...\n            print(f'{key}: {value}')\n    print(f'{\"*\" * len(header)}\\n')\n\n\ndef get_available_binary_name(dataset_name='unknown'):\n    prefix = f'gat_{dataset_name}'\n\n    def valid_binary_name(binary_name):\n        pattern = re.compile(rf'{prefix}_[0-9]{{6}}\\.pth')\n        return re.fullmatch(pattern, binary_name) is not None\n\n    valid_binary_names = list(filter(valid_binary_name, os.listdir(BINARIES_PATH)))\n    if len(valid_binary_names) > 0:\n        last_binary_name = sorted(valid_binary_names)[-1]\n        new_suffix = int(last_binary_name.split('.')[0][-6:]) + 1  # increment by 1\n        return f'{prefix}_{str(new_suffix).zfill(6)}.pth'\n    else:\n        return f'{prefix}_000000.pth'","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:32:16.526433Z","iopub.execute_input":"2024-03-23T13:32:16.526759Z","iopub.status.idle":"2024-03-23T13:32:16.538092Z","shell.execute_reply.started":"2024-03-23T13:32:16.526731Z","shell.execute_reply":"2024-03-23T13:32:16.536960Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import argparse\n\n\ndef get_training_args():\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\"--num_of_epochs\", type=int, help=\"number of training epochs\", default=200)\n    parser.add_argument(\"--patience_period\", type=int, help=\"number of epochs with no improvement on val before terminating\", default=100)\n    parser.add_argument(\"--lr\", type=float, help=\"model learning rate\", default=5e-3)\n    parser.add_argument(\"--weight_decay\", type=float, help=\"L2 regularization on model weights\", default=0)\n    parser.add_argument(\"--should_test\", type=bool, help='should test the model on the test dataset?', default=True)\n    parser.add_argument(\"--force_cpu\", type=bool, help='use CPU if your GPU is too small', default=False)\n    parser.add_argument(\"--dataset_name\", type=type(\"PPI\"), help='dataset to use for training', default=\"PPI\")\n\n    parser.add_argument(\"--console_log_freq\", type=int, help=\"log to output console (epoch) freq (None for no logging)\", default=10)\n    parser.add_argument(\"--checkpoint_freq\", type=int, help=\"checkpoint model saving (epoch) freq (None for no logging)\", default=5)\n\n    parser.add_argument(\"--batch_size\", type=int, help='number of graphs in a batch', default=2)\n\n    # Logging/debugging/checkpoint related (helps a lot with experimentation)\n    parser.add_argument(\"--enable_tensorboard\", type=bool, help=\"enable tensorboard logging\", default=False)\n    args = parser.parse_args('')\n\n    \n    gat_config = {\n        \"num_of_layers\": 3,  \n        \"num_heads_per_layer\": [4, 4, 6],  \n        \"num_features_per_layer\": [PPI_NUM_INPUT_FEATURES, 64, 64, PPI_NUM_CLASSES], \n        \"add_skip_connection\": True, \n        \"bias\": True,  #\n        \"dropout\": 0.0,  \n    }\n\n    training_config = dict()\n    for arg in vars(args):\n        training_config[arg] = getattr(args, arg)\n    training_config['ppi_load_test_only'] = False  # load both train/val/test data loaders (don't change it)\n\n    # Add additional config information\n    training_config.update(gat_config)\n\n    return training_config","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:32:16.540577Z","iopub.execute_input":"2024-03-23T13:32:16.541009Z","iopub.status.idle":"2024-03-23T13:32:16.557031Z","shell.execute_reply.started":"2024-03-23T13:32:16.540978Z","shell.execute_reply":"2024-03-23T13:32:16.555964Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"get_training_args()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:32:16.558320Z","iopub.execute_input":"2024-03-23T13:32:16.558671Z","iopub.status.idle":"2024-03-23T13:32:16.576900Z","shell.execute_reply.started":"2024-03-23T13:32:16.558645Z","shell.execute_reply":"2024-03-23T13:32:16.575999Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'num_of_epochs': 200,\n 'patience_period': 100,\n 'lr': 0.005,\n 'weight_decay': 0,\n 'should_test': True,\n 'force_cpu': False,\n 'dataset_name': 'PPI',\n 'console_log_freq': 10,\n 'checkpoint_freq': 5,\n 'batch_size': 2,\n 'enable_tensorboard': False,\n 'ppi_load_test_only': False,\n 'num_of_layers': 3,\n 'num_heads_per_layer': [4, 4, 6],\n 'num_features_per_layer': [50, 64, 64, 121],\n 'add_skip_connection': True,\n 'bias': True,\n 'dropout': 0.0}"},"metadata":{}}]},{"cell_type":"code","source":"import time\n\n\ndef train_gat(config):\n    \n    global BEST_VAL_MICRO_F1, BEST_VAL_LOSS\n\n    #\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and not config['force_cpu'] else \"cpu\")\n\n    data_loader_train, data_loader_val, data_loader_test = load_graph_data(config, device)\n\n    gat = GAT(\n        num_of_layers=config['num_of_layers'],\n        num_heads_per_layer=config['num_heads_per_layer'],\n        num_features_per_layer=config['num_features_per_layer'],\n        add_skip_connection=config['add_skip_connection'],\n        bias=config['bias'],\n        dropout=config['dropout'],\n        \n    ).to(device)\n\n    loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n    optimizer = Adam(gat.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n\n    main_loop = get_main_loop(\n        config,\n        gat,\n        loss_fn,\n        optimizer,\n        config['patience_period'],\n        time.time())\n\n    BEST_VAL_MICRO_F1, BEST_VAL_LOSS, PATIENCE_CNT = [0, 0, 0]  # reset vars used for early stopping\n\n    for epoch in range(config['num_of_epochs']):\n        main_loop(phase=LoopPhase.TRAIN, data_loader=data_loader_train, epoch=epoch)\n\n        # Validation loop\n        with torch.no_grad():\n            try:\n                main_loop(phase=LoopPhase.VAL, data_loader=data_loader_val, epoch=epoch)\n            except Exception as e:  # \"patience has run out\" exception :O\n                print(str(e))\n                break \n   \n    if config['should_test']:\n        micro_f1 = main_loop(phase=LoopPhase.TEST, data_loader=data_loader_test)\n        config['test_perf'] = micro_f1\n\n        print('*' * 50)\n        print(f'Test micro-F1 = {micro_f1}')\n    else:\n        config['test_perf'] = -1\n\n    # Save the latest GAT in the binaries directory\n    torch.save(\n        get_training_state(config, gat),\n        os.path.join(BINARIES_PATH, get_available_binary_name(config['dataset_name']))\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:32:16.578271Z","iopub.execute_input":"2024-03-23T13:32:16.578576Z","iopub.status.idle":"2024-03-23T13:32:16.591863Z","shell.execute_reply.started":"2024-03-23T13:32:16.578550Z","shell.execute_reply":"2024-03-23T13:32:16.590870Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n\ndef get_main_loop(config, gat, sigmoid_cross_entropy_loss, optimizer, patience_period, time_start):\n\n    device = next(gat.parameters()).device  # fetch the device info from the model instead of passing it as a param\n\n    def main_loop(phase, data_loader, epoch=0):\n        global BEST_VAL_MICRO_F1, BEST_VAL_LOSS, PATIENCE_CNT, writer\n\n        # \n        if phase == LoopPhase.TRAIN:\n            gat.train()\n        else:\n            gat.eval()\n\n        for batch_idx, (node_features, gt_node_labels, edge_index) in enumerate(data_loader):\n            \n            edge_index = edge_index.to(device)\n            node_features = node_features.to(device)\n            gt_node_labels = gt_node_labels.to(device)\n\n            graph_data = (node_features, edge_index)\n\n           # [0] the node_features part of the data (index 1 contains the edge_index)\n            # shape = (N, C) where N is the number of nodes in the batch and C is the number of classes (121 for PPI)\n            nodes_unnormalized_scores = gat(graph_data)[0]\n\n           \n            loss = sigmoid_cross_entropy_loss(nodes_unnormalized_scores, gt_node_labels)\n\n            if phase == LoopPhase.TRAIN:\n                optimizer.zero_grad() \n                loss.backward()  \n                optimizer.step() \n\n            # C\n\n            # If the unnormalized score is bigger than 0 that means that sigmoid would have a value higher than 0.5\n            # (by sigmoid's definition) and thus we have predicted 1 for that label otherwise we have predicted 0.\n            pred = (nodes_unnormalized_scores > 0).float().cpu().numpy()\n            gt = gt_node_labels.cpu().numpy()\n            micro_f1 = f1_score(gt, pred, average='micro')\n\n            #\n            # Logging\n            #\n\n            global_step = len(data_loader) * epoch + batch_idx\n            if phase == LoopPhase.TRAIN:\n                # Log metrics\n                if config['enable_tensorboard']:\n                    writer.add_scalar('training_loss', loss.item(), global_step)\n                    writer.add_scalar('training_micro_f1', micro_f1, global_step)\n\n                if config['console_log_freq'] is not None and epoch % config['console_log_freq'] == 0 and batch_idx == 0:\n                    print(f'GAT training: time elapsed= {(time.time() - time_start):.2f} [s] |'\n                          f' epoch={epoch + 1} | batch={batch_idx + 1} | train micro-F1={micro_f1}.')\n\n                if config['checkpoint_freq'] is not None and (epoch + 1) % config['checkpoint_freq'] == 0 and batch_idx == 0:\n                    ckpt_model_name = f'gat_{config[\"dataset_name\"]}_ckpt_epoch_{epoch + 1}.pth'\n                    config['test_perf'] = -1  # test perf not calculated yet, note: perf means main metric micro-F1 here\n                    torch.save(get_training_state(config, gat), os.path.join(CHECKPOINTS_PATH, ckpt_model_name))\n\n            elif phase == LoopPhase.VAL:\n                if config['enable_tensorboard']:\n                    writer.add_scalar('val_loss', loss.item(), global_step)\n                    writer.add_scalar('val_micro_f1', micro_f1, global_step)\n\n                # Log to console\n                if config['console_log_freq'] is not None and epoch % config['console_log_freq'] == 0 and batch_idx == 0:\n                    print(f'GAT validation: time elapsed= {(time.time() - time_start):.2f} [s] |'\n                          f' epoch={epoch + 1} | batch={batch_idx + 1} | val micro-F1={micro_f1}')\n\n               \n                if micro_f1 > BEST_VAL_MICRO_F1 or loss.item() < BEST_VAL_LOSS:\n                    BEST_VAL_MICRO_F1 = max(micro_f1, BEST_VAL_MICRO_F1)  # keep track of the best validation micro_f1 so far\n                    BEST_VAL_LOSS = min(loss.item(), BEST_VAL_LOSS)  # and the minimal loss\n                    PATIENCE_CNT = 0  \n                else:\n                    PATIENCE_CNT += 1  \n\n                if PATIENCE_CNT >= patience_period:\n                    raise Exception('Stopping the training')\n\n            else:\n                return micro_f1  # \n\n    return main_loop  ","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:32:16.593165Z","iopub.execute_input":"2024-03-23T13:32:16.593553Z","iopub.status.idle":"2024-03-23T13:32:17.651525Z","shell.execute_reply.started":"2024-03-23T13:32:16.593516Z","shell.execute_reply":"2024-03-23T13:32:17.650712Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_gat(get_training_args())","metadata":{"execution":{"iopub.status.busy":"2024-03-23T13:33:35.021589Z","iopub.execute_input":"2024-03-23T13:33:35.022380Z","iopub.status.idle":"2024-03-23T13:39:30.005505Z","shell.execute_reply.started":"2024-03-23T13:33:35.022344Z","shell.execute_reply":"2024-03-23T13:39:30.004440Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Loading train graph 1 to CPU. It has 1767 nodes and 34085 edges.\nLoading train graph 2 to CPU. It has 1377 nodes and 31081 edges.\nLoading train graph 3 to CPU. It has 2263 nodes and 61907 edges.\nLoading train graph 4 to CPU. It has 2339 nodes and 67769 edges.\nLoading train graph 5 to CPU. It has 1578 nodes and 37740 edges.\nLoading train graph 6 to CPU. It has 1021 nodes and 19237 edges.\nLoading train graph 7 to CPU. It has 1823 nodes and 46153 edges.\nLoading train graph 8 to CPU. It has 2488 nodes and 72878 edges.\nLoading train graph 9 to CPU. It has 591 nodes and 8299 edges.\nLoading train graph 10 to CPU. It has 3312 nodes and 109510 edges.\nLoading train graph 11 to CPU. It has 2401 nodes and 66619 edges.\nLoading train graph 12 to CPU. It has 1878 nodes and 48146 edges.\nLoading train graph 13 to CPU. It has 1819 nodes and 47587 edges.\nLoading train graph 14 to CPU. It has 3480 nodes and 110234 edges.\nLoading train graph 15 to CPU. It has 2794 nodes and 88112 edges.\nLoading train graph 16 to CPU. It has 2326 nodes and 62188 edges.\nLoading train graph 17 to CPU. It has 2650 nodes and 79714 edges.\nLoading train graph 18 to CPU. It has 2815 nodes and 88335 edges.\nLoading train graph 19 to CPU. It has 3163 nodes and 97321 edges.\nLoading train graph 20 to CPU. It has 3021 nodes and 94359 edges.\nLoading valid graph 21 to CPU. It has 3230 nodes and 100676 edges.\nLoading valid graph 22 to CPU. It has 3284 nodes and 104758 edges.\nLoading test graph 23 to CPU. It has 3224 nodes and 103872 edges.\nLoading test graph 24 to CPU. It has 2300 nodes and 63628 edges.\nGAT training: time elapsed= 0.13 [s] | epoch=1 | batch=1 | train micro-F1=0.37762325037508926.\nGAT validation: time elapsed= 1.79 [s] | epoch=1 | batch=1 | val micro-F1=0.42000751450388\nGAT training: time elapsed= 16.81 [s] | epoch=11 | batch=1 | train micro-F1=0.7456791909474361.\nGAT validation: time elapsed= 18.35 [s] | epoch=11 | batch=1 | val micro-F1=0.743185891640439\nGAT training: time elapsed= 33.03 [s] | epoch=21 | batch=1 | train micro-F1=0.895537873965627.\nGAT validation: time elapsed= 34.55 [s] | epoch=21 | batch=1 | val micro-F1=0.7588828870840297\nGAT training: time elapsed= 49.70 [s] | epoch=31 | batch=1 | train micro-F1=0.8858810337368334.\nGAT validation: time elapsed= 51.17 [s] | epoch=31 | batch=1 | val micro-F1=0.8620717271868142\nGAT training: time elapsed= 66.47 [s] | epoch=41 | batch=1 | train micro-F1=0.9195409480149789.\nGAT validation: time elapsed= 68.21 [s] | epoch=41 | batch=1 | val micro-F1=0.88888435124116\nGAT training: time elapsed= 83.55 [s] | epoch=51 | batch=1 | train micro-F1=0.9484770335261729.\nGAT validation: time elapsed= 84.86 [s] | epoch=51 | batch=1 | val micro-F1=0.9171029389517108\nGAT training: time elapsed= 99.92 [s] | epoch=61 | batch=1 | train micro-F1=0.9494854218078786.\nGAT validation: time elapsed= 101.35 [s] | epoch=61 | batch=1 | val micro-F1=0.9282478775632996\nGAT training: time elapsed= 116.27 [s] | epoch=71 | batch=1 | train micro-F1=0.9505837785881888.\nGAT validation: time elapsed= 117.90 [s] | epoch=71 | batch=1 | val micro-F1=0.9399705967812583\nGAT training: time elapsed= 132.70 [s] | epoch=81 | batch=1 | train micro-F1=0.9616140511737495.\nGAT validation: time elapsed= 134.32 [s] | epoch=81 | batch=1 | val micro-F1=0.9430207114363147\nGAT training: time elapsed= 149.55 [s] | epoch=91 | batch=1 | train micro-F1=0.9684158604070434.\nGAT validation: time elapsed= 151.01 [s] | epoch=91 | batch=1 | val micro-F1=0.9450903198442648\nGAT training: time elapsed= 166.34 [s] | epoch=101 | batch=1 | train micro-F1=0.9623367295665332.\nGAT validation: time elapsed= 167.82 [s] | epoch=101 | batch=1 | val micro-F1=0.9524892470460276\nGAT training: time elapsed= 182.80 [s] | epoch=111 | batch=1 | train micro-F1=0.9747756719946973.\nGAT validation: time elapsed= 184.35 [s] | epoch=111 | batch=1 | val micro-F1=0.9496136145630125\nGAT training: time elapsed= 199.48 [s] | epoch=121 | batch=1 | train micro-F1=0.9848336294297585.\nGAT validation: time elapsed= 200.91 [s] | epoch=121 | batch=1 | val micro-F1=0.9567860713167119\nGAT training: time elapsed= 215.84 [s] | epoch=131 | batch=1 | train micro-F1=0.9844278640406893.\nGAT validation: time elapsed= 217.50 [s] | epoch=131 | batch=1 | val micro-F1=0.9600317434302744\nGAT training: time elapsed= 232.30 [s] | epoch=141 | batch=1 | train micro-F1=0.983973778019455.\nGAT validation: time elapsed= 233.84 [s] | epoch=141 | batch=1 | val micro-F1=0.9389986422624748\nGAT training: time elapsed= 249.21 [s] | epoch=151 | batch=1 | train micro-F1=0.9740272901229784.\nGAT validation: time elapsed= 250.80 [s] | epoch=151 | batch=1 | val micro-F1=0.9611037003672649\nGAT training: time elapsed= 266.40 [s] | epoch=161 | batch=1 | train micro-F1=0.9904026469205187.\nGAT validation: time elapsed= 267.80 [s] | epoch=161 | batch=1 | val micro-F1=0.962411725487573\nGAT training: time elapsed= 283.01 [s] | epoch=171 | batch=1 | train micro-F1=0.9858925498953056.\nGAT validation: time elapsed= 284.59 [s] | epoch=171 | batch=1 | val micro-F1=0.9635138175085223\nGAT training: time elapsed= 299.28 [s] | epoch=181 | batch=1 | train micro-F1=0.9846661429693636.\nGAT validation: time elapsed= 300.71 [s] | epoch=181 | batch=1 | val micro-F1=0.9619828262519131\nGAT training: time elapsed= 315.40 [s] | epoch=191 | batch=1 | train micro-F1=0.9832692066614592.\nGAT validation: time elapsed= 316.88 [s] | epoch=191 | batch=1 | val micro-F1=0.9641194737148765\n**************************************************\nTest micro-F1 = 0.9785256000421215\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
